{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datathon.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MXzBoureDiCc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Report Google Doc](https://docs.google.com/document/d/1CWJNf2I3JUKnVm02NwIsNZxq3Y5GMNvZSBCoo-qSc7M/edit?ts=5bcb4b11)\n",
        "\n",
        "# What does a county's chemical contaminants say about its industrial activity?\n",
        "\n",
        "## We need a profile of each county's industrial activity.\n",
        "We will build a profile of each county for a given year by concatenating a vector representing a county's water usage in 2010 and their occupation statistics for the given year.\n",
        "We normalize the water usage data for population and droughts during 2010 by  predicting water usage based on population and drought, and subtracting these predictions from the water usage data. This will give us a baseline measurement of a county's industrial activity in 2010.\n",
        "We will also need to remove socioeconomic patterns from the industrial occupation data. We do this by performing PCA on water usage+occupation+earnings+education data and remove the eigenvectors heavily corresponding to earnings+education patterns.\n",
        "We will then concatenate the water usage data and the occupation data into a single feature vector for each year representing the county's industrial profile for that year. We normalize every column to a mean and variance of 1.\n",
        "## We can then correlate the profile with chemical contaminant levels\n",
        "We will build a regression model to measure what an increase/decrease in contaminants say about industrial activity and vise versa. We will then cluster chemicals based on how they relate to industrial profiles (similarity of regression coefficients).\n",
        "We will then build a regression model from contaminants to changes in industrial activity (instead of the value at a given year). We then perform the same covariance and clustering as before.\n",
        "\n",
        "## Statistics:\n",
        "### Step 1\n",
        "- Build regression model from historical education attainment and the 2010 population to water usage. Subtract this inferred value from water usage.\n",
        "- Calculate covariance between occupation and water, earnings, and education.\n",
        "- Perform PCA on water usage, occupation, earnings and education (normalizing first).\n",
        "- Recompute occupation data with SES-heavy components removed.\n",
        "- Recalculate covariance between occupation and water, earnings, and education.\n",
        "- Concatenate occupation vector and water usage data (duplicating water usage data for each year).\n",
        "\n",
        "### Step 2\n",
        "- Build a regression model from industrial vector to chemical contaminants data (normalizing first).\n",
        "- Calculate contaminant predictions after perturbing regression and vise-versa.\n",
        "- Cluster chemicals based on their regression coefficients.\n",
        "- Repeat except using year-wise change in industrial vector (y_n+1 - y_n) instead of y_n.\n",
        "\n",
        "## Visualizations/graphs:\n",
        "- Covariance matrix of water usage, drought and population before and after normalizing water usage data.\n",
        "- Covariance matrix of occupation and peripherals before and after PCA.\n",
        "- Visualize PCA removal of SES.\n",
        "- Feature importances of the industrial vector.\n",
        "- Visualize effect of perturbing regression?\n",
        "- Cluster map visualizing similarity of regression coefficients for each chemical."
      ]
    },
    {
      "metadata": {
        "id": "GKnMr7zruQP-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1: Build industrial feature vector"
      ]
    },
    {
      "metadata": {
        "id": "oRVZ9CpZ-D4y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "water_df = pd.read_csv(\"water_usage.csv\")\n",
        "water_dict_df = pd.read_csv(\"water_usage_dictionary.csv\")\n",
        "industry_df = pd.read_csv(\"industry_occupation.csv\", encoding = \"latin1\")\n",
        "education_df = pd.read_csv(\"education_attainment.csv\", encoding = \"latin1\")\n",
        "earnings_df = pd.read_csv(\"earnings.csv\", encoding = \"latin1\")\n",
        "chemicals_df = pd.read_csv(\"chemicals.csv\", encoding = \"latin1\")\n",
        "droughts_df = pd.read_csv(\"droughts.csv\", encoding = \"latin1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cceuqwh8BWGl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_droughts_df = droughts_df.copy()\n",
        "\n",
        "def normalize(labels_df):\n",
        "  global new_droughts_df, water_df\n",
        "  droughts_df = new_droughts_df\n",
        "  \"\"\" Normalize any labels_df with fips for 2010 using drought and population \"\"\"\n",
        "  droughts_df['valid_start'] = pd.to_datetime(droughts_df['valid_start'])\n",
        "  droughts_df['valid_end'] = pd.to_datetime(droughts_df['valid_end'])\n",
        "\n",
        "  droughts_df = droughts_df[droughts_df['valid_end'].dt.year >= 2010]\n",
        "  droughts_df = droughts_df[droughts_df['valid_start'].dt.year <= 2010]\n",
        "  droughts_df = droughts_df[[\"fips\", \"none\", \"d0\", \"d1\", \"d2\", \"d3\", \"d4\"]]\n",
        "\n",
        "  features_df = water_df[[\"fips\", \"population\"]].merge(droughts_df, on=\"fips\").fillna(0)\n",
        "\n",
        "  features_df = features_df.merge(labels_df.fillna(0), on=\"fips\")\n",
        "\n",
        "  fips = [x for x in features_df[\"fips\"]]\n",
        "  features = [x[1].tolist() for x in features_df[[\"none\", \"d0\", \"d1\", \"d2\", \"d3\", \"d4\", \"population\"]].iterrows()]\n",
        "  labels = [x[1].tolist() for x in features_df.drop(columns=[\"fips\", \"none\", \"d0\", \"d1\", \"d2\", \"d3\", \"d4\", \"population\"]).iterrows()]\n",
        "\n",
        "  water_normalizer = linear_model.LinearRegression()\n",
        "  water_normalizer.fit(features, labels)\n",
        "  labels = np.subtract(np.array(labels), np.array(water_normalizer.predict(features)))\n",
        "\n",
        "  return {k: v for k, v in zip(fips, labels)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5auCUmb2Bqeq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "M = {}\n",
        "\n",
        "# intersec_fips\n",
        "intersec_fips = list(set(water_df.fips).intersection(set(industry_df.fips)))\n",
        "\n",
        "# drop invariant cols\n",
        "drop_industry_df = industry_df.drop(['geo_id','county'], axis=1).dropna(0)\n",
        "drop_water_dict = normalize(water_df.drop(['state','state_fips','county','county_fips','year','population'], axis=1))\n",
        "\n",
        "for fips in intersec_fips:\n",
        "    ind_vecs = drop_industry_df[:][drop_industry_df.fips == fips]\n",
        "    ind_years = ind_vecs.year.values\n",
        "    #print(ind_years)\n",
        "    ind_vecs = ind_vecs.drop(['year','fips'],axis = 1)\n",
        "    ind_vecs = ind_vecs.astype(float).divide(ind_vecs[\"total_employed\"].astype(float), axis=0)\n",
        "    ind_vecs = ind_vecs.values\n",
        "    #print(ind_vecs)\n",
        "    wat_vec = drop_water_dict[fips]\n",
        "    wat_vec = [0 if np.isnan(v) else v for v in wat_vec]\n",
        "    for idx, vec in enumerate(ind_vecs):\n",
        "        vec_nan = [0 if np.isnan(v) else v for v in vec]\n",
        "        M[str(fips) + str(ind_years[idx])] = wat_vec + vec_nan\n",
        "col_names = list(drop_water_df.columns.values)[1:]+list(industry_df.columns.values[2:-2])\n",
        "col_name_dict = {name: idx for idx, name in enumerate(col_names)}\n",
        "county_dict = M\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ServeQzgAopA"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 2: Correlating industrial activity and contaminant levels."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GlsMdYasAno3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "''' Import necessary libraries '''\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jw4qqCQTws91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Generate the contamination dataset \"\"\"\n",
        "\n",
        "changed_chemicals_df = chemicals_df.copy()\n",
        "changed_chemicals_df[\"fips\"] = changed_chemicals_df[\"fips\"].astype(str) + changed_chemicals_df[\"year\"].astype(str)\n",
        "# Grab dataset from dataframe\n",
        "contamination_dict = {}\n",
        "for _, row in changed_chemicals_df.iterrows():\n",
        "  if row[\"fips\"] not in contamination_dict:\n",
        "    contamination_dict[row[\"fips\"]] = {}\n",
        "  contamination_dict[row[\"fips\"]][row[\"chemical_species\"] + \"(\" + row[\"unit_measurement\"] + \")\"] = row[\"value\"]\n",
        "\n",
        "# Build a map from feature idx to chemical\n",
        "chemical_index_map = []\n",
        "for fips, value in contamination_dict.items():\n",
        "  for chem, _ in value.items():\n",
        "    if chem not in chemical_index_map:\n",
        "      chemical_index_map.append(chem)\n",
        "\n",
        "# Build an index of overall idx to fips\n",
        "idx_to_fips = []\n",
        "contamination_vectors = []\n",
        "for fips in contamination_dict.keys():\n",
        "  idx_to_fips.append(fips)\n",
        "  feature_vec = [0 for _ in range(len(chemical_index_map))]\n",
        "  for idx, chem in enumerate(chemical_index_map):\n",
        "    if chem not in contamination_dict[fips]:\n",
        "      feature_vec[idx] = 0\n",
        "    else:\n",
        "      feature_vec[idx] = contamination_dict[fips][chem]\n",
        "  contamination_vectors.append(feature_vec)\n",
        "\n",
        "\n",
        "# Normalize contamination vectors to mean, var of 1.\n",
        "# We store the reverse scaler in reverse_contaminant_normalization(x).\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "scaler.fit(contamination_vectors)\n",
        "reverse_contaminant_normalization = lambda x: x * scaler.var_ + scaler.mean_\n",
        "contamination_vectors = scaler.transform(contamination_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4gYCTg4OtJmz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Clean up industrial activity dataset for our uses \"\"\"\n",
        "\n",
        "county_data = [(key, v) for key, v in county_dict.items()]\n",
        "\n",
        "# Drop contaminants we don't have county vecs for.\n",
        "county_fips = [x[0] for x in county_data]\n",
        "new_idx_to_fips = [x for x in idx_to_fips if x in county_fips]\n",
        "new_contamination_vectors = [contamination_vectors[idx_to_fips.index(x)] for x in new_idx_to_fips]\n",
        "idx_to_fips = new_idx_to_fips\n",
        "contamination_vectors = new_contamination_vectors\n",
        "\n",
        "# Build county vectors\n",
        "county_fips = {x[0]: x[1] for x in county_data}\n",
        "county_vectors = []\n",
        "for fips in idx_to_fips:\n",
        "  county_vectors.append(county_fips[fips])\n",
        "\n",
        "scala = StandardScaler(with_mean=True, with_std=True)\n",
        "scala.fit(county_vectors)\n",
        "county_vectors = scala.transform(county_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4C3nGVhIrudW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Given a dataset of feature vectors chemical contaminant levels, try to predict\n",
        "the vector representing the industrial activity for a given county at a given\n",
        "year. Cluster the chemicals based on the similarity of their correlations to\n",
        "the industrial activity.\n",
        "\"\"\"\n",
        "\n",
        "chem_to_county = linear_model.LinearRegression()\n",
        "chem_to_county.fit(contamination_vectors, county_vectors)\n",
        "coefs = chem_to_county.coef_\n",
        "    \n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(coefs)\n",
        "cluster_assignments = {}\n",
        "coefffs = {}\n",
        "for i, chem_name in contamination_idx_to_chemical.items():\n",
        "  cluster_assignments[chem_name] = kmeans.labels_[i]\n",
        "  coefffs[chem_name] = sklearn.manifold.TSNE().fit_transform(coefs[i])\n",
        "print(cluster_assignments)\n",
        "print(coefffs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jg7OOIp3qUBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Given a dataset of feature vectors representing the industrial activity\n",
        "for a given county at a given year, try to predict the chmemical contaminant\n",
        "level. What happens to contaminant levels when we perturb the county vectors?\n",
        "\"\"\"\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "def run(cols):\n",
        "  global county_vectors, contamination_vectors, county_to_chem\n",
        "\n",
        "  # PUT DELTA VEC HERE:\n",
        "  zero_vec = [0 for _ in range(len(county_vectors[0]))]\n",
        "  delta_vec = [0 for _ in range(len(county_vectors[0]))]\n",
        "  for n in [*cols]:\n",
        "    delta_vec[col_name_dict[n]] = 1\n",
        "  delta_vec = np.nan_to_num(delta_vec / scala.var_)\n",
        "\n",
        "  county_to_chem = linear_model.Ridge(alpha = .5)\n",
        "  county_to_chem.fit(county_vectors, contamination_vectors)\n",
        "\n",
        "  results = county_to_chem.predict([delta_vec])[0] - county_to_chem.predict([zero_vec])[0]\n",
        "  effect = {contamination_idx_to_chemical[i]: value\n",
        "            for i, value in enumerate(results * scaler.var_)}\n",
        "  print(effect)\n",
        "  return effect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0AB8Gpdeb8L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cols \n",
        "ind_cols = col_names[20:29]\n",
        "irr_cols = col_names[29:36]\n",
        "mining_cols = col_names[62:71]\n",
        "livestock_cols = col_names[50:53]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7OAK4Q0-eiQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "run(ind_cols)\n",
        "run(irr_cols)\n",
        "run(mining_cols)\n",
        "run(livestock_cols)\n",
        "\n",
        "data = [[ 66386, 174296,  75131, 577908,  32015],\n",
        "        [ 58230, 381139,  78045,  99308, 160454],\n",
        "        [ 89135,  80552, 152558, 497981, 603535],\n",
        "        [ 78415,  81858, 150656, 193263,  69638],\n",
        "        [139361, 331509, 343164, 781380,  52269]]\n",
        "\n",
        "columns = (\"Industrial\", \"Irrigation\", \"Mining\", \"Livestock\")\n",
        "rows =  run(irr_cols).keys()\n",
        "\n",
        "values = np.arange(0, 2500, 500)\n",
        "value_increment = 1000\n",
        "\n",
        "# Get some pastel shades for the colors\n",
        "colors = plt.cm.BuPu(np.linspace(0, 0.5, len(rows)))\n",
        "n_rows = len(data)\n",
        "\n",
        "index = np.arange(len(columns)) + 0.3\n",
        "bar_width = 0.4\n",
        "\n",
        "# Initialize the vertical-offset for the stacked bar chart.\n",
        "y_offset = np.zeros(len(columns))\n",
        "\n",
        "# Plot bars and create text labels for the table\n",
        "cell_text = []\n",
        "for row in range(n_rows):\n",
        "    plt.bar(index, data[row], bar_width, bottom=y_offset, color=colors[row])\n",
        "    y_offset = y_offset + data[row]\n",
        "    cell_text.append(['%1.1f' % (x / 1000.0) for x in y_offset])\n",
        "# Reverse colors and text labels to display the last value at the top.\n",
        "colors = colors[::-1]\n",
        "cell_text.reverse()\n",
        "\n",
        "# Add a table at the bottom of the axes\n",
        "the_table = plt.table(cellText=cell_text,\n",
        "                      rowLabels=rows,\n",
        "                      rowColours=colors,\n",
        "                      colLabels=columns,\n",
        "                      loc='bottom')\n",
        "\n",
        "# Adjust layout to make room for the table:\n",
        "plt.subplots_adjust(left=0.2, bottom=0.2)\n",
        "\n",
        "plt.ylabel(\"Loss in ${0}'s\".format(value_increment))\n",
        "plt.yticks(values * value_increment, ['%d' % val for val in values])\n",
        "plt.xticks([])\n",
        "plt.title('Loss by Disaster')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_QEG4nVcgIPI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5fu7n_LhCKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FLZanE--gOPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X = columns \n",
        "pca = PCA()\n",
        "pca.fit(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ig-s9_Bdg23k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def pca_normalize(county_dict):\n",
        "  earnings_df = pd.read_csv(\"earnings.csv\", encoding = \"latin1\")\n",
        "  earnings_df[\"fips\"] = earnings_df[\"fips\"].astype(str) + earnings_df[\"year\"].astype(str)\n",
        "  earnings_df.drop([\"pub_admin\", \"year\", \"county\", \"geo_id\"], axis=1)\n",
        "  x = copy.deepcopy(county_dict)\n",
        "  feat_n = None\n",
        "\n",
        "  for row in earnings_df.iterrows():\n",
        "    print(x)\n",
        "    feat_n = len(list(row)[1:])\n",
        "    x[row[\"fips\"]] += list(row)[1:]\n",
        "    \n",
        "  keys = x.keys()\n",
        "  data = [x[key] for key in keys]\n",
        "  pca = PCA()\n",
        "  pca.fit(data)\n",
        "  pca.components_.pop(IND)\n",
        "  pca.singular_values_.pop(IND)\n",
        "  y = sum([x * y for x, y in zip(pca.components_, pca.singular_values_)])\n",
        "  y = y[:-feat_n]\n",
        "  \n",
        "\n",
        "new_county_dict = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JgVr2wTXg3yl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list(next(earnings_df.iterrows())[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Eu9fJq5hdyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "array = np.array\n",
        "\n",
        "A = {'Uranium(micrograms/L)': array([-0.00584229,  0.72542275,  0.08034749,  0.02117824, -0.03213056,\n",
        "      -0.04028493]), 'Arsenic(micrograms/L)': array([ 0.0008375 , -0.02308488,  0.07815932,  0.04418895, -0.03681452,\n",
        "       0.06958854]), 'DEHP(micrograms/L)': array([-0.02464186, -0.12505089, -0.09959427, -0.06536517,  0.02902133,\n",
        "       0.08917486]), 'Nitrates(milligrams/L)': array([ 9.02643105e-04,  1.15423081e+00,  9.82211576e-02,  2.61018523e-02,\n",
        "      -6.28437209e-02, -5.42133463e-02]), 'Halo-Acetic Acid(micrograms/L)': array([ 0.00754173, -0.39016013,  0.08838049, -0.03296108, -0.07166619,\n",
        "       0.03815253]), 'Trihalomethane(micrograms/L)': array([ 0.00165637,  1.0730999 ,  0.10397881,  0.02175341, -0.06810205,\n",
        "      -0.04833789])}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MxtX_8gEksop",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "keys = A.keys()\n",
        "v = [A[k] for k in keys]\n",
        "\n",
        "x = sklearn.manifold.TSNE().fit_transform(v)\n",
        "\n",
        "for key, v in zip(keys, x):\n",
        "  print(key, v/100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYKJbI0_k0pl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}